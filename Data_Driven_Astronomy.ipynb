{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since joins are very common when working with databases, SQL provides its own JOIN operator for this purpose. We can use this operator in a variety of ways to achieve a variety of join types. To emulate the equi-joins we saw on the previous slides, we can use the JOIN operator with two different keywords:\n",
    "\n",
    "- JOIN ... USING:<br>\n",
    "Specifying a field of attribute to test for equality\n",
    "- JOIN ... ON:<br>\n",
    "Specifying a condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Handling cursor data\n",
    "\n",
    "As you've seen in the last problem, the data from SQL queries in Psycopg2 is returned in form of Python lists. In the last problem, you requested the full Star and Planet table, which returned a list of n tuples of length m, where m is the number of columns and n is the number of rows in these tables.\n",
    "\n",
    "A list of tuples cannot be used in the same way as e.g. a 2D Numpy array. For example, the following method of indexing to access the first element will not work:\n",
    "\n",
    "\n",
    "``` Python\n",
    "a = [(1, 2, 3), (4, 5, 6)]\n",
    "print(a[0, 0])\n",
    "```\n",
    "\n",
    "Instead, we have to use the [] operator twice: first to access the first list element, i.e. the first tuple, and then to access the first element in that tuple:\n",
    "\n",
    "``` Python\n",
    "a = [(1, 2, 3), (4, 5, 6)]\n",
    "print(a[0][0])\n",
    "```\n",
    "\n",
    "\n",
    "Using this indexing method, we can then access every individual data element. This allows us to, e.g. extract entire columns of the data by looping over the rows. The following code snippet shows an example which extracts the t_eff column from the full Star table and appends it to a new list:\n",
    "\n",
    "``` Python\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(dbname='db', user='grok')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT * FROM Star')\n",
    "records = cursor.fetchall()\n",
    "\n",
    "t_eff = []\n",
    "for row in records:\n",
    "  t_eff.append(row[1])\n",
    "print(t_eff)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Data types in SQL and Python\n",
    "\n",
    "Now we've seen how to work with query results, we can have a closer look at the data itself. In the previous activity, we learned about different data types in SQL when we were setting up tables.\n",
    "\n",
    "How do these SQL data types get converted into Python types?\n",
    "\n",
    "Let's have a look at the Planet table's data types. We can use a query which selects all columns but only a single row:\n",
    "``` sql\n",
    "SELECT * FROM Planet LIMIT 1;\n",
    "```\n",
    "\n",
    "In Python, this query will return a list containing a single tuple. We can loop over the entries of this tuple and call the type function to determine the data types:\n",
    "``` Python\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(dbname='db', user='grok')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT * FROM Planet LIMIT 1;')\n",
    "records = cursor.fetchall()\n",
    "\n",
    "for col in records[0]:\n",
    "    print(type(col))\n",
    "```\n",
    "\n",
    "The type conversion of these types is straight-forward: SQL's SMALLINT and INTEGER get converted to Python integers, CHAR and VARCHAR to Python strings, and FLOAT to Python floats.\n",
    "\n",
    "Check out the Psycopg2 documentation when you want to learn about type conversion in more detail. \n",
    "\n",
    "\n",
    "## Writing data into NumPy arrays\n",
    "\n",
    "Once we have the numerical data from the database in Python, we can write them into NumPy arrays.\n",
    "Since we're often dealing with data of different types in databases, it is important to remember that while Python lists and tuples can hold data of different types, NumPy arrays cannot.\n",
    "\n",
    "To convert a Python list into a simple NumPy array, we must ensure that the list only contains data of one type. Other than that, SQL results can easily be loaded into NumPy arrays:\n",
    "\n",
    "``` Python\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "conn = psycopg2.connect(dbname='db', user='grok')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT radius FROM Star;')\n",
    "records = cursor.fetchall()\n",
    "\n",
    "array = np.array(records)\n",
    "print(array.shape)\n",
    "print(array.mean())\n",
    "print(array.std())\n",
    "\n",
    "```\n",
    "(66, 1)\n",
    "\n",
    "0.886863636364\n",
    "\n",
    "0.237456527847\n",
    "\n",
    "Once the data is stored in NumPy arrays, we have access to all of NumPy's functionality to manipulate and analyse our data. One thing that we can now easily do is for example calculating a median.\n",
    "\n",
    "## A proper median\n",
    "\n",
    "Write a function called column_stats which calculates the mean and median of a selected column in either Star or Planet table. For this, let your function take two string arguments:\n",
    "\n",
    "    1 - the name of the table;\n",
    "    2 - the name of the column.\n",
    "\n",
    "and have it return the mean and median (in this order) of the selected column.\n",
    "When you call your function on, for example, the t_eff column of the Star table, the function call and return should look like this:\n",
    "\n",
    "``` Python\n",
    "column_stats('Star', 't_eff')\n",
    "(5490.681818181818, 5634.0)\n",
    "```\n",
    "\n",
    "You can compare your calculation with the pure SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT AVG(t_eff) FROM Star;\n",
    "```\n",
    "```\n",
    "+-----------------------+\n",
    "|          avg          |\n",
    "+-----------------------+\n",
    "| 5490.6818181818181818 |\n",
    "+-----------------------+\n",
    "```\n",
    "\n",
    "``` Python\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "conn = psycopg2.connect(dbname='db', user='grok')\n",
    "cursor = conn.cursor()\n",
    "def column_stats(indb, cols):\n",
    "  cursor.execute('SELECT '+cols+' FROM '+indb+';')\n",
    "  records = cursor.fetchall()\n",
    "  array = np.array(records)\n",
    "  return (array.mean(), np.median(array))\n",
    "```\n",
    "\n",
    "\n",
    "## SQL vs. Python\n",
    "\n",
    "In this course you've learned two different approaches to dealing with data. Which you choose for a particular project depends on a variety of factors including the questions you're posing of the data or whether you're using a public database or catalogue.\n",
    "\n",
    "We have seen that SQL is convenient to use for a lot of things â€“ but exactly how convenient is it? Can we do the same thing in Python?\n",
    "\n",
    "Let's go through a few problems in which we implement typical SQL queries from the previous activities in Python. We will start of with a simple query and add a new element in each problem.\n",
    "\n",
    "\n",
    "## Simple queries in Python 1/3\n",
    "\n",
    "Your first task is to replicate the following SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT kepler_id, radius\n",
    "FROM Star\n",
    "WHERE radius > 1.0;\n",
    "```\n",
    "\n",
    "The data is stored in stars.csv, with the kepler_id in the first column and the radius in the last.\n",
    "\n",
    "Write a function called query which takes the CSV filename as an argument and returns the data in a 2-column NumPy array. For example, this small CSV file:\n",
    "```\n",
    "stars.csv\n",
    "10666592,6350,1.991\n",
    "10682541,5339,0.847\n",
    "10797460,5850,1.04\n",
    "```\n",
    "your query function should work as follows:\n",
    "\n",
    "``` Python\n",
    "query('stars.csv')\n",
    "--> array([[  1.06665920e+07   1.99100000e+00]\n",
    "       [  1.07974600e+07   1.04000000e+00]])\n",
    "```\n",
    "The numerical data gets automatically converted to floats in this procedure, don't worry if it doesn't look like the SQL output. \n",
    "\n",
    "\n",
    "``` Python\n",
    "import numpy as np\n",
    "def query(incsv):\n",
    "  data = np.loadtxt(incsv, usecols=(0,2), delimiter=',',unpack=True)\n",
    "  nn = np.where(data[1,:] > 1.0)\n",
    "  hh = np.transpose(data[:,nn[0]])\n",
    "  return hh\n",
    "\n",
    "# You can use this to test your code\n",
    "# Everything inside this if-statement will be ignored by the automarker\n",
    "if __name__ == '__main__':\n",
    "  # Compare your function output to the SQL query\n",
    "  result = query('stars.csv')\n",
    "```\n",
    "\n",
    "\n",
    "## Simple queries in Python 2/3\n",
    "\n",
    "Let's add another element to our query. Sort the resulting table in ascending order to match the result you would get with:\n",
    "\n",
    "```sql\n",
    "SELECT kepler_id, radius\n",
    "FROM Star\n",
    "WHERE radius > 1.0\n",
    "ORDER BY radius ASC;\n",
    "```\n",
    "\n",
    "You can use your results from the last problem and then build up on that. Again, the function should be named query and it should take the filename as argument.\n",
    "\n",
    "- Hint\n",
    "\n",
    "You can use NumPy's argsort function to solve this problem. Take a look at how it works:\n",
    "\n",
    "``` Python\n",
    "import numpy as np\n",
    "a = np.array([3, 1, 2, 0])\n",
    "b = np.argsort(a)\n",
    "print(b)\n",
    "print(a[b])\n",
    "```\n",
    "\n",
    "It returns the indices of the unsorted array a in their new, sorted positions. You can pass this returned array b to the original array a to rearrange its entries. \n",
    "\n",
    "**Result**\n",
    "\n",
    "``` Python\n",
    "import numpy as np\n",
    "def query(incsv):\n",
    "  data = np.loadtxt(incsv, usecols=(0,2), delimiter=',',unpack=True)\n",
    "  nn = np.where(data[1,:] > 1.0)\n",
    "  hh = np.transpose(data[:,nn[0]])\n",
    "  b = np.argsort(hh[:,1])\n",
    "  return hh[b,:]\n",
    "```\n",
    "\n",
    "## Simple queries in Python 3/3\n",
    "\n",
    "Let's add yet another element to our query. Join the Star table with the Planet table and calculate the size ratio, i.e. planet radius / star radius for each star-planet pair. Your query function should produce the same result as the SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT p.radius/s.radius AS radius_ratio\n",
    "FROM Planet AS p\n",
    "INNER JOIN star AS s USING (kepler_id)\n",
    "WHERE s.radius > 1.0\n",
    "ORDER BY p.radius/s.radius ASC;\n",
    "```\n",
    "\n",
    "You can use your results from the last problem and then build up on that. The function must be named query, but this time it should take two filenames as arguments, for the stars and planets.\n",
    "\n",
    "In planets.csv, the first column is the kepler_id and the second last column is the radius.\n",
    "\n",
    "Your function should be a column vector of ratios, like this:\n",
    "\n",
    "```Python\n",
    ">>> query('stars.csv', 'planets.csv')\n",
    "array([[  0.48798799],\n",
    "       [  0.8260447 ],\n",
    "       [  0.96209913],\n",
    "       [  1.1556384 ],\n",
    "       [  1.30403969],\n",
    "       ...\n",
    "```\n",
    "\n",
    "**Hint**\n",
    "\n",
    "You may need to use a nested loop to compare each Planet's kepler_id against each Star's kepler_id. Once you've found a match and the star's radius is larger than one, you can append the ratio to the results list or array.\n",
    "\n",
    "```Python\n",
    "# Write your query function here\n",
    "import numpy as np\n",
    "def query(incsv1, incsv2):\n",
    "  data1 = np.loadtxt(incsv1, usecols=(0,2), delimiter=',',unpack=True)\n",
    "  data2 = np.loadtxt(incsv2, usecols=(0,5), delimiter=',',unpack=True) \n",
    "  ratio = np.asarray([])\n",
    "  for i in range(len(data1[1,:])):\n",
    "    for j in range(len(data2[1,:])):\n",
    "      if ((data1[0,i] == data2[0,j]) & (data1[1,i] > 1.0)) :\n",
    "        ratio = np.append(ratio, [data2[1,j]/data1[1,i]])\n",
    "  b = np.argsort(ratio)\n",
    "  return ratio[b].reshape(len(ratio),1)\n",
    "\n",
    "# You can use this to test your code\n",
    "# Everything inside this if-statement will be ignored by the automarker\n",
    "if __name__ == '__main__':\n",
    "  # Compare your function output to the SQL query\n",
    "  result = query('stars.csv', 'planets.csv')\n",
    "  \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## The right tool for every job\n",
    "\n",
    "The last three problems showed that Python is straight-forward to use for simple queries, but gets a lot more difficult as queries become more complex. The last question on joins was especially hard to implement in Python, whereas it's relatively simple in SQL.\n",
    "\n",
    "This shouldn't be surprising, as that's exactly what SQL is designed for and what we should use for these problems. There are good reasons though for why you might not want to drop Python completely for database-related work.\n",
    "\n",
    "One important thing to consider is that SQL is developed for accessing data and the built-in functions support only basic mathematical operations. Beyond that it gets very complicated.\n",
    "\n",
    "A good example for this is the calculation of the median, which we have done a couple of times in Python. There are no built-in functions for the median in SQL however, and doing it by hand in SQL gets pretty complicated. We haven't even covered enough SQL to understand how to implement a median, but if you're interested, check out this Postgresql article which shows examples of how a median could be implemented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this activity, we're going to use decision trees to determine the redshifts of galaxies from their photometric colours. We'll use galaxies where accurate spectroscopic redshifts have been calculated as our gold standard. We will learn how to assess the accuracy of the decision trees predictions and have a look at validation of our model.\n",
    "\n",
    "We will also have a quick look at how this problem might be approached without using machine learning. This will highlight some of the limitations of the classical approach and demonstrate why a machine learning approach is ideal for this type of problem.\n",
    "\n",
    "If you want to run your code offline, you can download the full NumPy dataset for this activity here.\n",
    "\n",
    "This activity is based on the scikit-learn example on Photometric Redshifts of Galaxies.\n",
    "\n",
    "\n",
    "\n",
    "## Magnitudes and colours\n",
    "\n",
    "We will be using flux magnitudes from the Sloan Digital Sky Survey (SDSS) catalogue to create colour indices. Flux magnitudes are the total flux (or light) received in five frequency bands (u, g, r, i and z).\n",
    "image of sdss filters\n",
    "\n",
    "\n",
    "<img src=\"files/week5/plot_sdss_filters_11.png\",width=450, height=400 >\n",
    "\n",
    "\n",
    "*The astronomical colour (or colour index)* is the difference between the magnitudes of two filters, i.e. *u - g* or *i - z*.\n",
    "\n",
    "This index is one way to characterise the colours of galaxies. For example, if the *u-g* index is high then the object is brighter in ultra violet frequencies than it is in visible green frequencies.\n",
    "\n",
    "Colour indices act as an approximation for the spectrum of the object and are useful for classifying stars into different types.\n",
    "\n",
    "## What data do we need?\n",
    "\n",
    "To calculate the redshift of a distant galaxy, the most accurate method is to observe the optical emission lines and measure the shift in wavelength. However, this process can be time consuming and is thus infeasible for large samples.\n",
    "\n",
    "For many galaxies we simply don't have spectroscopic observations.\n",
    "\n",
    "Instead, we can calculate the redshift by measuring the flux using a number of different filters and comparing this to models of what we expect galaxies to look like at different redshifts.\n",
    "\n",
    "In this activity, we will use machine learning to obtain photometric redshifts for a large sample of galaxies. We will use the colour indices (u-g, g-i, r-i and i-z) as our input and a subset of sources with spectroscopic redshifts as the training dataset.\n",
    "\n",
    "\n",
    "## Decision tree algorithms\n",
    "\n",
    "Decision trees are a tool that can be used for both classification and regression. In this module we will look at regression, however, in the next module we will see how they can be used as classifiers.\n",
    "\n",
    "Decision trees map a set of input features to their corresponding output targets. This is done through a series of individual decisions where each decision represents a node (or branching) of the tree.\n",
    "\n",
    "The following figure shows the decision tree our proverbial robot tennis player Robi used in the lectures to try and decide whether to play tennis on a particular day.\n",
    "Tennis decision tree\n",
    "\n",
    "<img src=\"week5/tennis.png\",width=450, height=400 >\n",
    "\n",
    "Should we play tennis?\n",
    "\n",
    "Each node represents a decision that the robot needs to make (or assess) to reach a final decision. In this example, the decision tree will be passed a set of input features (Outlook, Humidity and Wind) and will return an output of whether to play or not.\n",
    "\n",
    "\n",
    "## Decision trees for regression\n",
    "\n",
    "In decision trees for real-world tasks, each decision is typically more complex, involving measured values, not just categories.\n",
    "\n",
    "Instead of the input values for humidity being Normal or High and wind being Strong or Weak we might see a percentage between 0 and 100 for humidity and a wind speed in km/hr for wind. Our decisions might then be humidity < 40% or wind < 5 km/hr.\n",
    "\n",
    "The output of regression is a real number. So, instead of the two outputs of <font color='blue'>Play</font> and <font color='blue'>Don't Play </font>we have a probability of whether we will play that day.\n",
    "\n",
    "The decision at each branch is determined from the training data by the decision tree learning algorithm. Each algorithm employs a different metric (e.g. `Gini impurity` or `information gain`) to find the decision that splits the data most effectively.\n",
    "\n",
    "For now, just need to know that a decision tree is a series of decisions, each made on a single feature of the data. The end point of all the branches is a set of desired target values.\n",
    "\n",
    "\n",
    "## Decision trees in Python\n",
    "\n",
    "The inputs to our decision tree are the colour indices from photometric imaging and our output is a photometric redshift. Our training data uses accurate spectroscopic measurements.\n",
    "\n",
    "The decision tree will look something like the following.\n",
    "\n",
    "<img src=\"week5/decisiontree.png\",width=450, height=400 >\n",
    "\n",
    "We can see how our calculated colour indices are input as features at the top and through a series of decision nodes a target redshift value is reached and output.\n",
    "\n",
    "We will be using the Python machine learning library `scikit-learn` which provides several machine learning algorithms.\n",
    "\n",
    "The [`scikit-learn decision tree regression`](http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)  takes a set of input features and the corresponding target values, and constructs a decision tree model that can be applied to new data.\n",
    "\n",
    "## Sloan Digital Sky Survey data\n",
    "\n",
    "We have provided the Sloan data in NumPy binary format (.npy) in a file called sdss_galaxy_colors.npy. The Sloan data is stored in a NumPy structured array and looks like this:\n",
    "\n",
    "|u \t|g \t|r \t|i \t|z \t|... \t|redshift\n",
    "|---|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|19.84 \t|19.53 |\t19.47 \t|19.18 \t|19.11 \t|... \t|0.54|\n",
    "|19.86 \t|18.66 |\t17.84 \t|17.39 \t|17.14 \t|... \t|0.16|\n",
    "|... \t|... \t|... \t|... \t|... \t|... \t|...|\n",
    "|18.00 \t|17.81 \t|17.77 \t|17.73 \t|17.73 \t|... \t|0.47|\n",
    "\n",
    "It also include spec_class and redshift_err columns we don't need in this activity. The data can be loaded using:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "print(data[0])\n",
    "```\n",
    "The data[0] corresponds to the first row of the table above. Individual named columns can be accessed like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "print(data['u'])\n",
    "```\n",
    "\n",
    "Each flux magnitude column can be accessed in the data array as data['u'], data['g'] etc. The redshifts can accessed with data['redshift']. \n",
    "\n",
    "\n",
    "\n",
    "## Features and Targets\n",
    "\n",
    "Write a `get_features_targets` function that splits the training data into input `features` and their corresponding `targets`. In our case, the inputs are the 4 colour indices and our targets are the corresponding redshifts.\n",
    "\n",
    "Your function should return a tuple of:\n",
    "\n",
    "   - **features**: a NumPy array of dimensions m â¨‰ 4, where m is the number of galaxies;\n",
    "   - **targets**: a 1D NumPy array of length m, containing the redshift for each galaxy.\n",
    "\n",
    "The data argument will be the structured array described on the previous slide. The u flux magnitudes and redshifts can be accessed as a column with data['u'] and data['redshift'].\n",
    "\n",
    "The four features are the colours u - g, g - r, r - i and i - z. To calculate the first column of features, subtract the u and g columns, like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "print(data['u'] - data['g'])\n",
    "```\n",
    "\n",
    "The features for the first 2 galaxies in the example data should be:\n",
    "```python\n",
    "[[ 0.31476  0.0571   0.28991  0.07192]\n",
    " [ 1.2002   0.82026  0.45294  0.24665]]\n",
    "```\n",
    "\n",
    "And the first 2 targets should be:\n",
    "```python\n",
    "[ 0.539301   0.1645703]\n",
    "```\n",
    "\n",
    "## Hint:\n",
    "set up your features array with zeros\n",
    "\n",
    "You can set up the features array with zeros and then set each column to the corresponding calculated feature.\n",
    "```python\n",
    "features = np.zeros((data.shape[0], 4))\n",
    "features[:, 0] = data['u'] - data['g']\n",
    "```\n",
    "```python\n",
    "import numpy as np\n",
    "def get_features_targets(data):\n",
    "  # complete this function\n",
    "  features = np.transpose([data['u']-data['g'],data['g']-data['r'],data['r']-data['i'],data['i']-data['z']])\n",
    "  target = data['redshift']\n",
    "  return features,target\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # load the data\n",
    "  data = np.load('sdss_galaxy_colors.npy')\n",
    "  # call our function \n",
    "  features, targets = get_features_targets(data)\n",
    "  # print the shape of the returned arrays\n",
    "  print(features[:2])\n",
    "  print(targets[:2])\n",
    "```\n",
    "\n",
    "\n",
    "## Decision Tree Regressor\n",
    "\n",
    "We are now going to use our features and targets to train a decision tree and then make a prediction. We are going to use the [DecisionTreeRegressor](http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html) class from the `sklearn.tree` module.\n",
    "\n",
    "The decision tree regression learning algorithm is initialised with:\n",
    "```python\n",
    "dtr = DecisionTreeRegressor()\n",
    "```\n",
    "We will discuss some optimisations later in the activity, but for now we are just going to use the default values.\n",
    "\n",
    "To train the model, we use the fit method with the features and targets we created earlier:\n",
    "```python\n",
    "dtr.fit(features, targets)\n",
    "```\n",
    "The decision tree is now trained and ready to make a prediction:\n",
    "```python\n",
    "predictions = dtr.predict(features)\n",
    "```\n",
    "predictions is an array of predicted values corresponding to each of the input variables in the array.\n",
    "\n",
    "Your task is to put this together for our photometric redshift data. Copy your get_features_targets from the previous problem. Use the comments to guide your implementation.\n",
    "\n",
    "Finally, print the first 4 predictions. It should print this:\n",
    "\n",
    "```python\n",
    "[ 0.539301    0.1645703   0.04190006  0.04427702]\n",
    "```\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# copy in your get_features_targets function here\n",
    "def get_features_targets(data):\n",
    "  # complete this function\n",
    "  features = np.transpose([data['u']-data['g'],data['g']-data['r'],data['r']-data['i'],data['i']-data['z']])\n",
    "  target = data['redshift']\n",
    "  return features,target\n",
    "\n",
    "# load the data and generate the features and targets\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "features, targets = get_features_targets(data)\n",
    "  \n",
    "# initialize model\n",
    "dtr = DecisionTreeRegressor()\n",
    "# train the model\n",
    "dtr.fit(features, targets)\n",
    "# make predictions using the same features\n",
    "predictions = dtr.predict(features)\n",
    "# print out the first 4 predicted redshifts\n",
    "print(predictions[:4])\n",
    "```\n",
    "\n",
    "## Evaluating our results: accuracy\n",
    "\n",
    "So we trained a decision tree! Great...but how do we know if the tree is actually any good at predicting redshifts?\n",
    "\n",
    "In regression we compare the predictions generated by our model with the actual values to test how well our model is performing. The difference between the predicted values and actual values (sometimes referred to as residuals) can tell us a lot about where our model is performing well and where it is not.\n",
    "\n",
    "While there are a few different ways to characterise these differences, in this tutorial we will use the median of the differences between our predicted and actual values. This is given by:\n",
    "\n",
    "$$ med{\\_}diff = median\\left(\\left| Y_{i,pred} - Y_{i,act}\\right|\\right) $$\n",
    "\n",
    "Where $\\left|  .....  \\right|$ denotes the absolute value of the difference.\n",
    "\n",
    "\n",
    "\n",
    "## Calculating the median difference\n",
    "\n",
    "In this problem we will implement the function median_diff. The function should calculate the median residual error of our model, i.e. the median difference between our predicted and actual redshifts.\n",
    "\n",
    "The median_diff function takes two arguments â€“ the predicted and actual/target values. When we use this function later in the tutorial, these will corresponding to the predicted redshifts from our decision tree and their corresponding measured/target values. \n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "# write a function that calculates the median of the differences\n",
    "# between our predicted and actual values\n",
    "def median_diff(predicted, actual):\n",
    "  error = np.abs(predicted - actual)\n",
    "  return np.median(error)\n",
    "  \n",
    "```\n",
    "\n",
    "\n",
    "## Evaluating our results: validation\n",
    "\n",
    "We previously used the same data for training and testing our decision trees.\n",
    "\n",
    "This gives an unrealistic estimate of how accurate the model will be on previously unseen galaxies because the model has been optimised to get the best results on the training data.\n",
    "\n",
    "The simplest way to solve this problem is to split our data into training and testing subsets:\n",
    "\n",
    "```python\n",
    "# initialise and train the decision tree\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(train_features, train_targets)\n",
    "# get a set of prediction from the test input features\n",
    "predictions = dtr.predict(test_features)\n",
    "# compare the accuracy of the pediction againt the actual values\n",
    "print(calculate_rmsd(predictions, test_targets))\n",
    "```\n",
    "\n",
    "This method of validation is the most basic approach to validation and is called held-out validation. We will use the med_diff accuracy measure and hold-out validation in the next problem to assess the accuracy of our decision tree.\n",
    "\n",
    "\n",
    "# Validating our model\n",
    "\n",
    "In this problem, we will use median_diff from the previous question to validate the decision tree model. Your task is to complete the validate_model function.\n",
    "\n",
    "The function should split the features and targets into train and test subsets, like this 50:50 split for features:\n",
    "\n",
    "```python\n",
    "split = features.shape[0]//2\n",
    "train_features = features[:split]\n",
    "test_features = features[split:]\n",
    "```\n",
    "\n",
    "Your function should then use the training split (train_features and train_targets) to train the model.\n",
    "\n",
    "Finally, it should measure the accuracy of the model using median_diff on the test_targets and the predicted redshifts from test_features.\n",
    "\n",
    "The function should take 3 arguments:\n",
    "\n",
    "  - **model**: the decision tree regressor;\n",
    "  - **features** - the features for the data set;\n",
    "  - **targets** - The targets for the data set.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "keep features and targets together!\n",
    "When splitting the features and targets be careful to ensure that the train_features have the correct train_targets, i.e. train_features[0] corresponds to train_targets[0] etc.\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# paste your get_features_targets function here\n",
    "def get_features_targets(data):\n",
    "  # complete this function\n",
    "  features = np.transpose([data['u']-data['g'],data['g']-data['r'],data['r']-data['i'],data['i']-data['z']])\n",
    "  target = data['redshift']\n",
    "  return features,target\n",
    "\n",
    "# paste your median_diff function here\n",
    "def median_diff(predicted, actual):\n",
    "  error = np.abs(predicted - actual)\n",
    "  return np.median(error)\n",
    "\n",
    "# write a function that splits the data into training and testing subsets\n",
    "# trains the model and returns the prediction accuracy with median_diff\n",
    "def validate_model(model, features, targets):\n",
    "  # split the data into training and testing features and predictions\n",
    "  split = features.shape[0]//2\n",
    "  train_features = features[:split]\n",
    "  train_targets = targets[:split]  \n",
    "  test_features = features[split:]\n",
    "  test_targets = targets[split:]\n",
    "  # train the model\n",
    "  model.fit(train_features, train_targets) \n",
    "  # get the predicted_redshifts\n",
    "  predictions = model.predict(test_features)\n",
    "  # use median_diff function to calculate the accuracy\n",
    "  return median_diff(test_targets, predictions)\n",
    "\n",
    "```\n",
    "\n",
    "## Exploring the output tree\n",
    "\n",
    "But what does the decision tree actually look like?\n",
    "\n",
    "<img src=\"week5/decision_tree.jpeg\",width=650, height=600 >\n",
    "\n",
    "\n",
    "You can see how the decision is made at each node as well as the number of samples which reach that node. We won't go through how to make these plots in the tutorial, but you can download a demo script and data to try at home.\n",
    "\n",
    "The median of differences of ~ 0.02. This means that half of our galaxies have a error in the prediction of < 0.02, which is pretty good. One of the reason we chose the median of differences as our accuracy measure is that it gives a fair representation of the errors especially when the distribution of errors is skewed. The graph below shows the distribution of residuals (differences) for our model along with the median and interquartile values.\n",
    "\n",
    "<img src=\"week5/residuals.png\",width=450, height=400 >\n",
    "\n",
    "\n",
    "As you can tell the distribution is very skewed. We have zoomed in here, but the tail of the distribution goes all the way out to 6.\n",
    "\n",
    "\n",
    "## The effect of training set size\n",
    "\n",
    "The number of galaxies we use to train the model has a big impact on how accurate our predictions will be. This is the same with most machine learning methods: the more data that they are trained with, the more accurate their prediction will be.\n",
    "\n",
    "Here is how our median difference changes with training set size:\n",
    "\n",
    "\n",
    "|Training galaxies |\tmedian_diff|\n",
    "|:---:|:-----:|\n",
    "|50\t|0.048|\n",
    "|500\t|0.026|\n",
    "|5000\t|0.023|\n",
    "|50000\t|0.022|\n",
    "\n",
    "Understanding how the accuracy of the model changes with sample size is important to understanding the limitations of our model. We are approaching the accuracy limit of the decision tree model (for our redshift problem) with a training sample size of 25,000 galaxies.\n",
    "\n",
    "The only way we could further improve our model would be to use more features. This might include more colour indices or even the errors associated with the measured flux magnitudes.\n",
    "\n",
    "## Before machine learning\n",
    "\n",
    "Before machine learning, we would have tried to solve this problem with regression â€” by constructing an empirical model to predict how the dependent variable (redshift) varies with one or more independent variables (the colour indices).\n",
    "\n",
    "A plot of how the colours change with redshift tells us that it is quite a complex function, for example redshift versus `u - g`:\n",
    "\n",
    "<img src=\"week5/Redshift-U-G.png\",width=450, height=400  >\n",
    "\n",
    "One approach would be to construct a multi-variate non-linear regression model. Perhaps using a least squares fitting to try and determine the best fit parameters. The model would be quite complex; based on the above plot, a dampened inverse sine function would be a good starting point for such a model.\n",
    "\n",
    "While we could try such an approach the function would be overly complex and there is no guarantee that it would yield very promising results. Another approach would be to plot a colour-index vs colour-index plot using an additional colour scale to show redshift. The following plot is an example of such a plot.\n",
    "\n",
    "<img src=\"week5/Redshift-colour-colour.png\",width=450, height=400  >\n",
    "\n",
    "It shows that we get reasonably well defined regions where redshifts are similar. If we were to make a contour map of the redshifts in the colour index vs colour index space we would be able to get an estimate of the redshift for new data points based on a combination of their colour indices. This would lead to redshift estimates with significant uncertainties attached to them.\n",
    "\n",
    "In the next problem you will re-create the Colour-index vs Colour-index plot with redshift as colour bar.\n",
    "\n",
    "## Colour-Colour Redshift Plot\n",
    "\n",
    "Your task here is simply to try and re-create the previous plot.\n",
    "\n",
    "You should use the pyplot module of matplotlib which has already been imported and can be accessed through plt. In particular you can use the plt.scatter() function, with additional arguments s, c and cmap.\n",
    "\n",
    "We are interested in the u-g and r-i colour indices.\n",
    "\n",
    "You can make use of the plt.colorbar() function to show your scatter plots colour argument(c) to a colour bar on the side of the plot.\n",
    "\n",
    "Make sure you implement x and y labels and give your plot a title.\n",
    "Gotchas\n",
    "\n",
    "In order to get your plot working with colours you will need to set the optional parameter lw=number. There are a lot of small points and unless we set the linewidth to zero the line surrounding each point will block out the point itself.\n",
    "\n",
    "You should also specify the size of your plot points with the additional call arguments s=number. A reasonable place to start might be 1, but see what it looks like.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Complete the following to make the plot\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('sdss_galaxy_colors.npy')\n",
    "    # Get a colour map\n",
    "    cmap = plt.get_cmap('YlOrRd')\n",
    "\n",
    "    # Define our colour indexes u-g and r-i\n",
    "    ug = data['u']-data['g']\n",
    "    ri = data['r']-data['i']\n",
    "    # Make a redshift array\n",
    "    redshift = data['redshift']\n",
    "    # Create the plot with plt.scatter and plt.colorbar\n",
    "    plt.scatter(ug,ri, s=5, lw=0, c=redshift, cmap=cmap)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.set_ylabel('Redshift')\n",
    "    # Define your axis labels and plot title\n",
    "    plt.xlabel('Colour index u - g')\n",
    "    plt.title('Redshift (color) u - g versus r - i')\n",
    "    plt.ylabel('Colour index r - i')\n",
    "    # Set any axis limits\n",
    "    plt.xlim(-0.5,2.5)\n",
    "    plt.ylim(-0.5,1.0)\n",
    "    plt.show()\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this activity, we implemented some decision tree models to help predict the redshift of galaxies based on their measured colour indices. We learnt that there are several ways to assess the accuracy of the model and in our validations we used the median of the residuals.\n",
    "\n",
    "We touched on how the problem might be approached without machine learning and found that there isn't too much available that can be simply used.\n",
    "\n",
    "We also learnt why we need to cross validate the model and that this is done by splitting the data into seperate training and testing subsets. We will look at k-fold cross validation in the next tutorial; where the testing and training are changed to include various combinations of k seperate subsets.\n",
    "\n",
    "In the next tutorial we will also explore how decision trees have a tendency to overfit the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this activity, we will be using machine learning to classify galaxies into three types (ellipticals, spirals or galactic mergers) based on their observed properties.\n",
    "\n",
    "In the last activity you had a go at classifying galaxies by hand on the [Galaxy Zoo website](https://www.galaxyzoo.org/#/classify) which hopefully gave you some intuition for the dataset and how we can distinguish between the different types of galaxy.\n",
    "\n",
    "For our machine learning experiments, we are using the crowd-classified classes from Galaxy Zoo as the training data for our automatic decision tree classifier.\n",
    "\n",
    "We'll start by looking at how classification differs from regression. We will then implement some of the new features and parameters that we will use to reduce the dimensionality of the problem. We will also show you how to measure accuracy for classification problems, before extending our classifier to use random forests.\n",
    "\n",
    "If you would like to try this on your own machine, the data set can be downloaded [here](https://groklearning-cdn.com/modules/jSAg5N3xrpKtvpDtCtMTGV/galaxy_catalogue.npy).\n",
    "\n",
    "\n",
    "## Classification vs Regression\n",
    "\n",
    "In classification, the predictions are from a fixed set of classes, whereas in regression the prediction typically corresponds to a continuum of possible values.\n",
    "\n",
    "In regression, we measure accuracy by looking at the size of the differences between the predicted values and the actual values. In contrast, in classification problems a prediction can either be correct or incorrect. This makes measuring the accuracy of our model a lot simpler.\n",
    "\n",
    "In terms of implementation using decision trees, there is very little difference between classification and regression. The only notable difference is that our targets are classes rather than real values. When calculating the accuracy, we check whether the predicted class matches as the actual class.\n",
    "A note on decision tree regression\n",
    "\n",
    "In decision tree regression, the possible outputs are a finite set of values that correspond to the number of leaves/end points in the tree. Ideally we want as many points as possible to give a good approximation of the 'continuous' parameter space, whilst avoiding overfitting.\n",
    "\n",
    "## The Galaxy Zoo data\n",
    "\n",
    "In the last activity, you were a human classifier for the [Galaxy Zoo project](https://www.galaxyzoo.org/#/classify) and probably saw a wide range of galaxy types observed by the Sloan Digital Sky Survey. In this activity, we will limit our dataset to three types of galaxy: spirals, ellipticals and mergers.\n",
    "\n",
    "\n",
    "<img src='week6/Classes.png'>\n",
    "\n",
    "The galaxy catalogue we are using is a sample of galaxies where at least 20 human classifiers (such as yourself) have come to a consensus on the galaxy type.\n",
    "\n",
    "Examples of spiral and elliptical galaxies were selected where there was a unanimous classification. Due to low sample numbers, we included merger examples where at least 80% of human classifiers selected the merger class.\n",
    "\n",
    "We need this high quality data to train our classifier.\n",
    "\n",
    "## Deciding on features\n",
    "\n",
    "Just like in the regression activities, we need to decide on a set of key features that represent our data.\n",
    "\n",
    "While approaches exist that determine their own feature representation and use the raw pixel values as inputs, e.g. neural networks and deep learning, the majority of existing machine learning in astronomy requires an expert to design the feature set.\n",
    "\n",
    "In this activity we will be using a set of features derived from fitting images according to known galaxy profiles.\n",
    "\n",
    "Most of the features we use here are based on the five observed flux magnitudes from the Sloan Digital Sky Survey filters:\n",
    "\n",
    "<img src='week6/sdss_filters.png', width=600, height=400>\n",
    "\n",
    "\n",
    "## Selecting features\n",
    "\n",
    "The features that we will be using to do our galaxy classification are colour index, adaptive moments, eccentricities and concentrations. These features are provided as part of the SDSS catalogue.\n",
    "\n",
    "We briefly describe these below. Further information how they are calculated can be found [here](http://skyserver.sdss.org/dr7/en/help/docs/algorithm.asp).\n",
    "\n",
    "**Colour indices** are the same colours (u-g, g-r, r-i, and i-z) we used for regression. Studies of galaxy evolution tell us that spiral galaxies have younger star populations and therefore are 'bluer' (brighter at lower wavelengths). Elliptical galaxies have an older star population and are brighter at higher wavelengths ('redder').\n",
    "\n",
    "**Eccentricity** approximates the shape of the galaxy by fitting an ellipse to its profile. Eccentricity is the ratio of the two axis (semi-major and semi-minor). The De Vaucouleurs model was used to attain these two axis. To simplify our experiments, we will use the median eccentricity across the 5 filters. \n",
    "\n",
    "<img src='week6/eccentricity_example.png', width=400, height=400>\n",
    "\n",
    "\n",
    "## Selecting features - continued\n",
    "\n",
    "Adaptive moments also describe the shape of a galaxy. They are used in image analysis to detect similar objects at different sizes and orientations. We use the fourth moment here for each band.\n",
    "\n",
    "Concentration is similar to the luminosity profile of the galaxy, which measures what proportion of a galaxy's total light is emitted within what radius. A simplified way to represent this is to take the ratio of the radii containing 50% and 90% of the Petrosian flux.\n",
    "\n",
    "The Petrosian method allows us to compare the radial profiles of galaxies at different distances. If you are interested, you can [read more here](http://spiff.rit.edu/classes/phys443/lectures/gal_1/petro/petro.html) on the need for Petrosian approach.\n",
    "\n",
    "For these experiments, we will define concentration as:\n",
    "\n",
    "$$ \\textrm{conc}= \\frac{\\textrm{Petro}_{R50}}{\\textrm{Petro}_{R90}}$$\n",
    "\n",
    "We will use the concentration from the u, r and z bands. \n",
    "\n",
    "\n",
    "<img src='week6/concentration_example.png', width=400, height=400>\n",
    "\n",
    "\n",
    "## Data description\n",
    "\n",
    "We have extracted the SDSS and Galaxy Zoo data for 780 galaxies into a NumPy binary file. You can load the file using:\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('galaxy_catalogue.npy')\n",
    "```\n",
    "\n",
    "The data is a NumPy array of 780 records. Each record is a single galaxy. You can access the columns using field names. For example, to access the u-g colour filter for the first galaxy, you use:\n",
    "```python\n",
    "data[0]['u-g']\n",
    "```\n",
    "\n",
    "Here's a snippet that prints the field and values for the first galaxy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('galaxy_catalogue.npy')\n",
    "for name, value in zip(data.dtype.names, data[0]):\n",
    "  print('{:10} {:.6}'.format(name, value))\n",
    "```\n",
    "\n",
    "It shows the four colour features, median eccentricity, fourth adaptive moment of each filter, the Petrosian fluxes at a radius of 50% and 90% for the u, r and z filters, and finally the class:\n",
    "``` python\n",
    "    u-g        1.85765\n",
    "    g-r        0.67158\n",
    "    r-i        0.4231\n",
    "    i-z        0.3061\n",
    "    ecc        0.585428\n",
    "    m4_u       2.25195\n",
    "    m4_g       2.33985\n",
    "    m4_r       2.38065\n",
    "    m4_i       2.35974\n",
    "    m4_z       2.39553\n",
    "    petroR50_u 3.09512\n",
    "    petroR50_r 3.81892\n",
    "    petroR50_z 3.82623\n",
    "    petroR90_u 5.17481\n",
    "    petroR90_r 8.26301\n",
    "    petroR90_z 11.4773\n",
    "    class      merger\n",
    "```\n",
    "\n",
    "NumPy also allows you to access a field for all of the rows at once, i.e. a column, using the field's name:\n",
    "```python\n",
    "data['u-g']\n",
    "```\n",
    "\n",
    "\n",
    "## Splitting the train and test sets\n",
    "\n",
    "To start, we need to split the data into training and testing sets.\n",
    "\n",
    "Your task is to implement the `splitdata_train_test` function. It takes a NumPy array and splits it into a training and testing NumPy array based on the specified training fraction. The function takes two arguments and should return two values:\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `data`: the NumPy array containing the galaxies in the form described in the previous slide;\n",
    "- `fraction_training`: the fraction of the data to use for training. This will be a float between 0 and 1.\n",
    "\n",
    "The number of training rows should be truncated if necessary. For example, with a fraction of 0.67 and our 780 galaxies, the number of training rows is 780*0.67 = 722.6, which should be truncated to 722 using int. The remaining rows should be used for testing.\n",
    "\n",
    "### Return values\n",
    "\n",
    "- `training_set`: the first value is a NumPy array training set;\n",
    "- `testing_set`: the second value is a NumPy array testing set.\n",
    "\n",
    "Using the supplied driver code, and our input data and a fraction of 0.7, the program should print the following values:\n",
    "\n",
    "```python\n",
    "Number data galaxies: 780\n",
    "Train fraction: 0.7\n",
    "Number of galaxies in training set: 546\n",
    "Number of galaxies in testing set: 234\n",
    "```\n",
    "\n",
    "### Good practice: randomize the dataset order\n",
    "\n",
    "You shouldn't assume that the data has already been shuffled. If you look at `data['class']` you will see that the `merger`, `elliptical` and `spiral` examples appear together. Failing to shuffle the data will produce a very bad classifier! You can use:\n",
    "\n",
    "```python\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "```\n",
    "The first statement ensures the shuffle is the same for each experiment, so you get consistent results, the second shuffles the rows of the data array in place.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "def splitdata_train_test(data, fraction_training):\n",
    "  np.random.seed(0)\n",
    "  np.random.shuffle(data)\n",
    "  rows = int(len(data)*fraction_training)\n",
    "  return data[:rows], data[rows:]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "  # set the fraction of data which should be in the training set\n",
    "  fraction_training = 0.7\n",
    "  # split the data using your function\n",
    "  training, testing = splitdata_train_test(data, fraction_training)\n",
    "  # print the key values\n",
    "  print('Number data galaxies:', len(data))\n",
    "  print('Train fraction:', fraction_training)\n",
    "  print('Number of galaxies in training set:', len(training))\n",
    "  print('Number of galaxies in testing set:', len(testing))\n",
    "```\n",
    "\n",
    "## Generating features and targets\n",
    "\n",
    "Next, we generate features and targets for the decision tree.\n",
    "\n",
    "The `generate_features_targets` function is mostly complete. However, you need to calculate the concentration values for the u, r and z filters.\n",
    "\n",
    "Your task is to calculate the concentration for each filter from the 50% and 90% Petrosian radius measurements:\n",
    "\n",
    "$$ \\textrm{conc}= \\frac{\\textrm{Petro}_{R50}}{\\textrm{Petro}_{R90}}$$\n",
    "\n",
    "\n",
    "As described earlier, data has the following fields:\n",
    "\n",
    "- **colours**: u-g, g-r, r-i, and i-z;\n",
    "- **eccentricity**: ecc\n",
    "- **4th adaptive moments**: m4_u, m4_g, m4_r, m4_i, and m4_z;\n",
    "- **50% Petrosian**: petroR50_u, petroR50_r, petroR50_z;\n",
    "- **90% Petrosian**: petroR90_u, petroR90_r, petroR90_z.\n",
    "\n",
    "\n",
    "```python\n",
    "def generate_features_targets(data):\n",
    "  # complete the function by calculating the concentrations\n",
    "  targets = data['class']\n",
    "  features = np.empty(shape=(len(data), 13))\n",
    "  features[:, 0] = data['u-g']\n",
    "  features[:, 1] = data['g-r']\n",
    "  features[:, 2] = data['r-i']\n",
    "  features[:, 3] = data['i-z']\n",
    "  features[:, 4] = data['ecc']\n",
    "  features[:, 5] = data['m4_u']\n",
    "  features[:, 6] = data['m4_g']\n",
    "  features[:, 7] = data['m4_r']\n",
    "  features[:, 8] = data['m4_i']\n",
    "  features[:, 9] = data['m4_z']\n",
    "  # fill the remaining 3 columns with concentrations in the u, r and z filters\n",
    "  # concentration in u filter\n",
    "  features[:, 10] = data['petroR50_u']/data['petroR90_u']\n",
    "  # concentration in r filter\n",
    "  features[:, 11] = data['petroR50_r']/data['petroR90_r'] \n",
    "  # concentration in z filter\n",
    "  features[:, 12] =  data['petroR50_z']/data['petroR90_z']\n",
    "  return features, targets\n",
    "```\n",
    "\n",
    "## Train the decision tree classifier\n",
    "\n",
    "It is time to use the functions we wrote to split the data and generate the features, and then train a decision tree classifier.\n",
    "\n",
    "Your task is complete the `dtc_predict_actual` function by following the Python comments. The purpose of the function is to perform a held out validation and return the predicted and actual classes for later comparison.\n",
    "\n",
    "The function takes a single argument which is the full data set and should return two NumPy arrays containing the predicted and actual classes respectively.\n",
    "\n",
    "You will also need to copy your solutions from the previous two questions into the spaces allocated.\n",
    "\n",
    "```python\n",
    "def dtc_predict_actual(data):\n",
    "  # split the data into training and testing sets using a training fraction of 0.7\n",
    "  fraction_training = 0.7\n",
    "  training, testing = splitdata_train_test(data, fraction_training)\n",
    "  # generate the feature and targets for the training and test sets\n",
    "  # i.e. train_features, train_targets, test_features, test_targets\n",
    "  train_features,train_targets = generate_features_targets(training)\n",
    "  test_features, test_targets = generate_features_targets(testing) \n",
    "  # instantiate a decision tree classifier\n",
    "  model = DecisionTreeClassifier()\n",
    "  # train the classifier with the train_features and train_targets\n",
    "  model.fit(train_features,train_targets)\n",
    "  # get predictions for the test_features\n",
    "  predictions = model.predict(test_features)\n",
    "  # return the predictions and the test_targets\n",
    "  return predictions, test_targets\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "  predicted_class, actual_class = dtc_predict_actual(data)\n",
    "  # Print some of the initial results\n",
    "  print(\"Some initial results...\\n   predicted,  actual\")\n",
    "  for i in range(10):\n",
    "    print(\"{}. {}, {}\".format(i, predicted_class[i], actual_class[i]))\n",
    "```\n",
    "\n",
    "\n",
    "## Accuracy and model score\n",
    "\n",
    "The accuracy of classification problems is a lot simpler to calculate than for regression problems. The simplest measure is the fraction of objects that are correctly classified. That is\n",
    "\n",
    "\n",
    "$$ \\textrm{accuracy}= \\frac{\\textrm{# correct predictions}}{\\textrm{# predictions}}$$\n",
    "\n",
    "$$ \\textrm{accuracy}= \\frac{\\sum_{i=1}^{n}\\textrm{predicted}_{i} = \\textrm{actual}_{i}}{n}$$\n",
    "\n",
    "\n",
    "The accuracy measure is often called the model score. While the way of calculating the score can vary depending on the model, the accuracy is the most common for classification problems.\n",
    "\n",
    "Note: `sklearn` has methods to get the model score. Most models will have a `score` method which in the case of the decision tree classifier uses the above formula. The `cross_val_score` function in the `model_selection` module can be used to get k cross validated scores.\n",
    "\n",
    "\n",
    "## Confusion matrices\n",
    "\n",
    "In addition to an overall accuracy score, we'd also like to know where our model is going wrong. For example, were the incorrectly classified mergers mis-classified as spirals or ellipticals? To answer this type of question we use a confusion matrix. An example confusion matrix for our problem is shown below:\n",
    "\n",
    "\n",
    "<img src='week6/example_confusion.png', width=400, height=400>\n",
    "\n",
    "The x axis represents the predicted classes and the y axis represents the correct classes. The value in each cell is the number of examples with those predicted and actual classes. Correctly classified objects are along the diagonal of the matrix.\n",
    "\n",
    "So of the 260 actual spirals (correct class) in the data set, 198 are correctly predicted as spirals, 5 are incorrectly predicted as ellipticals and 57 are incorrectly predicted as mergers.\n",
    "\n",
    "The sum along each row or column can be used to get the totals of true and predicted classes. So for example, by summing each of the rows we can confirm that there are 260 mergers, 260 spirals and 260 ellipticals in the data set.\n",
    "\n",
    "\n",
    "## Accuracy in classification\n",
    "\n",
    "Your task is to complete the calculate_accuracy function. The function should calculate the accuracy: the fraction of predictions that are correct (i.e. the model score):\n",
    "\n",
    "$$ \\textrm{accuracy}= \\frac{\\textrm{# correct predictions}}{\\textrm{# predictions}}$$\n",
    "\n",
    "The function takes two arguments;\n",
    "\n",
    "- predicted: an array of the predicted class for each galaxy.\n",
    "- actual: an array of the actual class for each galaxy.\n",
    "\n",
    "The return value should be a float (between 0 and 1).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from support_functions import plot_confusion_matrix, generate_features_targets\n",
    "\n",
    "# Implement the following function\n",
    "def calculate_accuracy(predicted, actual):\n",
    "  correct = predicted[predicted == actual]\n",
    "  return float(len(correct)/len(actual))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "  # split the data\n",
    "  features, targets = generate_features_targets(data)\n",
    "\n",
    "  # train the model to get predicted and actual classes\n",
    "  dtc = DecisionTreeClassifier()\n",
    "  predicted = cross_val_predict(dtc, features, targets, cv=10)\n",
    "\n",
    "  # calculate the model score using your function\n",
    "  model_score = calculate_accuracy(predicted, targets)\n",
    "  print(\"Our accuracy score:\", model_score)\n",
    "\n",
    "  # calculate the models confusion matrix using sklearns confusion_matrix function\n",
    "  class_labels = list(set(targets))\n",
    "  model_cm = confusion_matrix(y_true=targets, y_pred=predicted, labels=class_labels)\n",
    "\n",
    "  # Plot the confusion matrix using the provided functions.\n",
    "  plt.figure()\n",
    "  plot_confusion_matrix(model_cm, classes=class_labels, normalize=False)\n",
    "  plt.show()\n",
    "```\n",
    "\n",
    "## Random forests classification\n",
    "\n",
    "So far we have used a single decision tree model. However, we can improve the accuracy of our classification by using a collection (or ensemble) of trees as known as a *random forest*.\n",
    "\n",
    "A random forest is a collection of decision trees that have each been independently trained using different subsets of the training data and/or different combinations of features in those subsets.\n",
    "\n",
    "When making a prediction, every tree in the forest gives its own prediction and the most common classification is taken as the overall forest prediction (in regression the mean prediction is used).\n",
    "\n",
    "\n",
    "## Advantages of random forest classifiers\n",
    "\n",
    "Random forests help to mitigate overfitting in decision trees.\n",
    "\n",
    "**Training data** is spread across decision trees. The subsets are created by taking random samples with replacement. This means that a given data point can be used in several subsets. (This is different from the subsets used in cross validation where each data point belongs to one subset).\n",
    "\n",
    "**Individual trees** are trained with different subsets of features. So in our current problem, one tree might be trained using eccentricity and another using concentration and the 4th adaptive moment. By using different combinations of input features you create expert trees that are can better identify classes by a given feature.\n",
    "\n",
    "**The sklearn random forest only uses the first form of sampling.**\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Your task here is to complete the `rf_predict_actual` function. It returns the predicted and actual classes for our galaxies using a random forest 10-fold with cross validation.\n",
    "\n",
    "You should use the `RandomForestClassifier` class from the `sklearn.ensemble` module. It can be instantiated with:\n",
    "```python\n",
    "rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "```\n",
    "\n",
    "`n_estimators` is the the number of decision trees in the forest.\n",
    "\n",
    "`rf_predict_actual` takes two arguments: the `data` used throughout this activity and the number of estimators (`n_estimators`) to be used in the random forest.\n",
    "\n",
    "The function should return two NumPy arrays containing the predicted and actual classes respectively.\n",
    "\n",
    "You can copy and paste the functions from previous questions. However, we have provided the `generate_features_targets` function in the support library.\n",
    "\n",
    "Use the `cross_val_predict` function from the model_selection module as we did in the last question.\n",
    "\n",
    "You can read its documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html). This approach allows us to get a prediction for every galaxy in the data set through cross validation. It also means that we don't need to manage the training and test sets.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from support_functions import generate_features_targets, plot_confusion_matrix, calculate_accuracy\n",
    "\n",
    "# complete this function to get predictions from a random forest classifier\n",
    "def rf_predict_actual(data, n_estimators):\n",
    "  # generate the features and targets\n",
    "  features, targets = generate_features_targets(data)\n",
    "  # instantiate a random forest classifier using n estimators\n",
    "  rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "  # get predictions using 10-fold cross validation with cross_val_predict\n",
    "  y_pred = cross_val_predict(rfc, features, targets)\n",
    "  # return the predictions and their actual classes\n",
    "  return y_pred, targets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "  # get the predicted and actual classes\n",
    "  number_estimators = 50              # Number of trees\n",
    "  predicted, actual = rf_predict_actual(data, number_estimators)\n",
    "\n",
    "  # calculate the model score using your function\n",
    "  accuracy = calculate_accuracy(predicted, actual)\n",
    "  print(\"Accuracy score:\", accuracy)\n",
    "\n",
    "  # calculate the models confusion matrix using sklearns confusion_matrix function\n",
    "  class_labels = list(set(actual))\n",
    "  model_cm = confusion_matrix(y_true=actual, y_pred=predicted, labels=class_labels)\n",
    "\n",
    "  # plot the confusion matrix using the provided functions.\n",
    "  plt.figure()\n",
    "  plot_confusion_matrix(model_cm, classes=class_labels, normalize=False)\n",
    "  plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "## Results discussion\n",
    "\n",
    "Did the random forest improve the accuracy of the model? The answer is yes â€“ we see a substantial increase in accuracy. When we look at the 10-fold cross validation results, we see that the random forest systematically out performs a single decision tree:\n",
    "\n",
    "|   |Random Forest\t|Decision Tree|\n",
    "|-----  |:--:|:---:| \n",
    "|Median score\t|0.865\t|0.775|\n",
    "|Mean score\t|0.867\t|0.792|\n",
    "|Standard deviation of scores\t|0.036\t|0.035|\n",
    "\n",
    "The random forest is around ~6-7% more accurate than a standard decision tree.\n",
    "\n",
    "Below is a side by side comparision of the confusion matrices from our galaxy catalogues. The first showing the results for the random forest classifier and the second for the decision tree classifier. There are improvements accross the board with the biggest improvement (percentage) being between ellipticals and spirals.\n",
    "\n",
    "<img src='week6/forest_tree_comparison.png', width=600, height=600>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "In this activity we have used decision tree classifiers to identify a galaxies type by a selection of derived features (e.g. eccentricity and concentration).\n",
    "\n",
    "We have learnt that asessing the models performance is a lot simpler with classification than it is with regression.\n",
    "\n",
    "Confusion matrices can be a useful tool to help understand where our model is over and under preforming with respect to each class.\n",
    "\n",
    "We started out with the goal of using decision trees to classify galaxies as one of three types. We found that we were able to achieve an accuracy of around 80% using decision trees which is very good when you consider the statistical accuracy of a random selection is 33% and the naive approach using only pixel values yielded ~64%.\n",
    "\n",
    "We were able to improve on the accuracy of the decision tree classifier by using a selection of them in ensemble learning with a random forest.\n",
    "\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "Congratulations, you've finished the final set of activities for the MOOC!\n",
    "\n",
    "If you've still got questions about any of the content, head to the forums to discuss with your fellow learners.\n",
    "\n",
    "Now head back to Coursera for the wrap up lecture. Please let us know if you have any feedback about the Grok Learning platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
